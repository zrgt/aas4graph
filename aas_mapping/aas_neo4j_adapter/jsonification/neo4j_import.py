import hashlib
import json
import logging
import os
import time
from copy import deepcopy
from os.path import join, isfile
from typing import Optional, List, Dict, Tuple, Any

from neo4j import Session
from neo4j.exceptions import TransientError, ClientError

from aas_mapping.aas_neo4j_adapter.base import BaseNeo4JClient, Neo4jModelConfig
from aas_mapping.aas_neo4j_adapter.utils import UploadStats

logger = logging.getLogger(__name__)

class JsonToNeo4jImporter(BaseNeo4JClient):
    def __init__(self, uri: str, user: str, password: Optional[str] = None, model_config: Neo4jModelConfig = None, **kwargs):
        super().__init__(uri=uri, user=user, password=password, model_config=model_config, **kwargs)
        self.uid_counter = 0

        # e.g. {HASH: uid}
        self.deduplicated_nodes: dict[str: int] = {}
        self.deduplicated_to_existing_uid_map: dict[int: int] = {}
        self.deduplicated_rels: set[str] = set()
        self.uid_to_internal_id: dict[str: int] = {}

    def _gen_unique_node_name(self) -> int:
        self.uid_counter += 1
        return self.uid_counter

    def _group_nodes_by_label(self, nodes: List[Dict]) -> Dict[Tuple[str], List[Dict]]:
        grouped = {}
        for node in nodes:
            labels = tuple(sorted(node.pop('labels')))
            if labels not in grouped:
                grouped[labels] = []
            grouped[labels].append(node)
        return grouped

    def _add_relationship(self, relationships: Dict[str, List], rel_type: str, from_uid: int, to_uid: int,
                          rel_props: Optional[dict] = None):
        """Add a relationship to the relationships dictionary."""
        if rel_type not in relationships:
            relationships[rel_type] = []
        if rel_props is None:
            rel_props = {}
        relationships[rel_type].append({
            'from_uid': from_uid,
            'to_uid': to_uid,
            'rel_props': rel_props
        })

    def _merge_relationships(self, target: Dict[str, List], source: Dict[str, List]):
        """Merge relationships from source into target."""
        for key, value in source.items():
            target.setdefault(key, []).extend(value)

    def _create_nodes(self, session: Session, grouped_nodes: Dict[Tuple[str], List[Dict]]) -> Dict[int, int]:
        """Create nodes in Neo4j and return uid to internal_id mapping."""
        create_nodes_query = """
        UNWIND keys($data) AS labelsString
        
        WITH split(labelsString, ",") AS labels, $data[labelsString] AS nodesProperties
        
        UNWIND nodesProperties AS nodeProperties
        CREATE (n:$(labels))
        SET n = nodeProperties
        
        RETURN elementId(n) AS internal_id, nodeProperties.uid AS uid
        """

        # Convert tuple keys to strings for Neo4j compatibility
        data_for_query = {
            ",".join(label_tuple): node_list
            for label_tuple, node_list in grouped_nodes.items()
        }

        for labels in grouped_nodes.keys():
            for label in labels:
                session.run(f"CREATE INDEX IF NOT EXISTS FOR (n:{label}) ON (n.uid)")

        uid_to_internal_id = {}
        # Create nodes
        result = session.run(create_nodes_query, data=data_for_query)
        for record in result:
            uid_to_internal_id[record['uid']] = record['internal_id']

        return uid_to_internal_id

    def _create_relationships(self, session: Session, relationships: Dict[str, List],
                              uid_to_internal_id: Dict[int, int], db_batch_size: int = 10000):
        """Create relationships in Neo4j."""
        created_rels = 0
        for rel_type, rel_list in relationships.items():
            for i in range(0, len(rel_list), db_batch_size):
                batch_rels = rel_list[i:i + db_batch_size]
                prepared_rels = []
                for rel in batch_rels:
                    try:
                        prepared_rels.append({
                            'from_id': uid_to_internal_id[rel['from_uid']],
                            'to_id': uid_to_internal_id[rel['to_uid']],
                            'rel_props': rel['rel_props']
                        })
                    except KeyError:
                        logger.warning(
                            f"Skipping relationship {rel_type} from {rel['from_uid']} to {rel['to_uid']} due to missing UID mapping.")
                        continue

                if prepared_rels:
                    # Create relationships in batch
                    create_rels_query = f"""
                    UNWIND $relationships AS rel
                    MATCH (from_node) WHERE elementId(from_node) = rel.from_id
                    MATCH (to_node) WHERE elementId(to_node) = rel.to_id
                    CREATE (from_node)-[r:{rel_type}]->(to_node)
                    SET r = rel.rel_props
                    RETURN count(*) as created
                    """

                    try:
                        result = session.run(create_rels_query, relationships=prepared_rels)
                        created = result.single()['created']
                        created_rels += created
                    except TransientError as e:
                        logger.error(f"Transient error during relationship creation batch: {e}")

        return created_rels

    def _deduplicate_nodes(self, grouped_nodes: dict[tuple[str], list[dict]]):
        for label_tuple, nodes in grouped_nodes.items():
            # Check if any of the labels in this tuple should be deduplicated
            if not any(lbl in self.model_config.deduplicated_object_types for lbl in label_tuple):
                continue

            filtered_nodes = []
            for node in nodes:
                # Deterministic JSON hash from properties
                node_copy = deepcopy(node)
                node_copy.pop("uid")
                hash_value = hashlib.sha256(json.dumps(node_copy, sort_keys=True).encode()).hexdigest()

                if hash_value in self.deduplicated_nodes:
                    # This node already exists (deduplicate)
                    existing_uid = self.deduplicated_nodes[hash_value]
                    self.deduplicated_to_existing_uid_map[node["uid"]] = existing_uid
                else:
                    node["hash"] = hash_value
                    # First time we see this node -> keep it
                    self.deduplicated_nodes[hash_value] = node["uid"]
                    filtered_nodes.append(node)

            # Replace node list with deduplicated version
            grouped_nodes[label_tuple] = filtered_nodes
        return grouped_nodes

    def _deduplicate_rels(self, relationships: dict[tuple[str], list[dict]]):
        # --- ðŸ”§ Rewrite relationships to use deduplicated UIDs ---
        for rel_types, rel_list in relationships.items():
            updated_rels = []
            for rel in rel_list:
                # Update UIDs if they exist in the deduplicated map
                rel["from_uid"] = self.deduplicated_to_existing_uid_map.get(rel["from_uid"], rel["from_uid"])
                rel["to_uid"] = self.deduplicated_to_existing_uid_map.get(rel["to_uid"], rel["to_uid"])

                # Deterministic JSON hash from properties
                hash_value = hashlib.sha256(json.dumps(rel, sort_keys=True).encode()).hexdigest()

                if hash_value not in self.deduplicated_rels:
                    self.deduplicated_rels.add(hash_value)
                    updated_rels.append(rel)

            # Replace with deduplicated and updated relationships
            rel_list[:] = updated_rels

        return relationships

    def _upload_nodes_and_relationships(self, nodes: List[Dict], relationships: Dict[str, List],
                                        stats: UploadStats = None,
                                        exist_uid_to_internal_id: Optional[Dict[int, int]] = None,
                                        db_batch_size: int = 1000) -> UploadStats:
        """Upload nodes and relationships to Neo4j in a single transaction."""
        if stats is None:
            stats = UploadStats()

        # Group nodes and filter relationships for this batch
        grouped_nodes = self._group_nodes_by_label(nodes)

        # --- ðŸ”§ Deduplication steps ---
        # TODO: the deduplication data should be saved late in a DB
        # TODO: Consider the deduplication while CRUD operations on AAS Server
        grouped_nodes = self._deduplicate_nodes(grouped_nodes)
        relationships = self._deduplicate_rels(relationships)

        # --- Continue with database operations ---
        with self.driver.session() as session:
            # 1. Create Nodes in Batches
            node_start_time = time.time()
            self.uid_to_internal_id.update(self._create_nodes(session, grouped_nodes))
            if exist_uid_to_internal_id:
                # Merge existing UID to internal ID mapping with newly created nodes
                self.uid_to_internal_id.update(exist_uid_to_internal_id)

            node_creation_time = time.time() - node_start_time
            stats.total_node_creation_time += node_creation_time
            node_count = sum(len(nodes) for nodes in grouped_nodes.values())
            stats.total_nodes_created += node_count
            logger.info(f"Created {node_count} nodes in {node_creation_time:.2f} seconds")

            # 2. Create Relationships in Batches
            rel_start_time = time.time()
            relationship_count = self._create_relationships(session, relationships, self.uid_to_internal_id, db_batch_size)
            relationship_creation_time = time.time() - rel_start_time
            stats.total_relationship_creation_time += relationship_creation_time
            stats.total_relationships_created += relationship_count
            logger.info(f"Created {relationship_count} relationships in {relationship_creation_time:.2f} seconds")

            # 3. Cleanup UIDs in Batches
            # self._cleanup_uids_in_session(session, list(uid_to_internal_id.values()), db_batch_size)

        # del grouped_nodes, uid_to_internal_id
        del grouped_nodes

        return stats

    def _cleanup_uids_in_session(self, session: Session, internal_ids: List[int], batch_size: int):
        """Removes `uid` property from nodes in batches within an existing session."""
        delete_query = """
        UNWIND $ids AS id
        MATCH (n)
        WHERE elementId(n) = id
        REMOVE n.uid
        """
        for i in range(0, len(internal_ids), batch_size):
            batch_ids = internal_ids[i:i + batch_size]
            try:
                session.run(delete_query, ids=batch_ids)
            except ClientError as e:
                logging.warning(f"Error during UID cleanup for batch: {e}")

    @staticmethod
    def identify_labels(obj: Dict):
        return ("Unknown",)

    def _process_dict(self, obj: Dict, node_properties: Optional[Dict[str, any]] = None) \
            -> Tuple[List[Dict], Dict[str, List]]:
        nodes = []
        relationships: dict[str, dict[str, str]] = {}
        node_properties = node_properties or {}

        node_uid = self._gen_unique_node_name()
        node_labels = self.identify_labels(obj)
        node_properties.update({
            'uid': node_uid,
            'labels': node_labels
        })

        # unpack the DICTS_TO_PROPERTY_LISTS
        list_of_dicts_prop_as_multiple_list_props = self.get_props_to_model_as_multiple_lists(node_labels)
        dict_prop_as_multiple_props = self.get_complex_props_to_model_as_multiple_simple_props(node_labels)

        for key, value in obj.items():
            if key in self.model_config.keys_to_ignore:
                continue
            elif key in list_of_dicts_prop_as_multiple_list_props:
                # BEFORE: keys = [{"type": "GlobalReference", "value": "0173-1#02-AAW001#001"}}, ...]
                # AFTER:  keys_type = ["GlobalReference", ...]
                #         keys_value = ["0173-1#02-AAW001#001", ...]
                if value:
                    for dict_key in value[0].keys():
                        node_properties[f"{key}_{dict_key}"] = [dict_[dict_key] for dict_ in value]
            elif key in dict_prop_as_multiple_props:
                if value:
                    child_nodes, child_rels = self._process_dict(value)
                    if len(child_nodes) > 1:
                        raise ValueError(f"The dict should have only one child node, got {len(child_nodes)}: {value}")

                    for sub_key, sub_value in child_nodes[0].items():
                        if sub_key not in ("uid", "labels"):
                            node_properties[f"{key}_{sub_key}"] = sub_value

                    rels = {
                        f"{key}_{rel_type}": {**rel, "from_uid": node_uid}
                        for rel_type, rel in child_rels.items()
                    }
                    self._merge_relationships(relationships, rels)

            elif isinstance(value, dict):
                child_nodes, child_rels = self._process_dict(value)
                nodes.extend(child_nodes)
                self._merge_relationships(relationships, child_rels)

                # Create relationship to the last created node
                if child_nodes:
                    self._add_relationship(relationships, key, node_uid, child_nodes[-1]['uid'])
                    if "Referable" in child_nodes[-1]['labels']:
                        self._add_relationship(relationships, "child", node_uid, child_nodes[-1]['uid'])

            elif isinstance(value, list):
                for i, item in enumerate(value, start=0):
                    if not isinstance(item, dict):
                        logger.warning(f"Unsupported type in list: {type(item)}")
                        continue

                    child_nodes, child_rels = self._process_dict(item)
                    nodes.extend(child_nodes)
                    self._merge_relationships(relationships, child_rels)

                    # Create relationship to direct child node, which is the last one
                    if child_nodes:
                        rel_props = {"is_list": True}

                        if self.model_config.all_list_item_relationships_have_index is True:
                            rel_props = {"list_index": i}
                        elif self.model_config.list_item_relationships_with_index:
                            for node_label in node_labels:
                                if key in self.model_config.list_item_relationships_with_index.get(node_label, []):
                                    rel_props = {"list_index": i}
                                    break
                        self._add_relationship(relationships, key, node_uid, child_nodes[-1]['uid'],
                                               rel_props=rel_props)

            else:
                node_properties[key] = value

        nodes.append(node_properties)
        return nodes, relationships

    def _process_json_data(self, json_data: Dict[str, Any]) -> Tuple[List[Dict], Dict[str, List]]:
        """
        Process JSON data into nodes and relationships.

        This method can be overloaded in child classes to process specific dicts, like AAS Environment serializations,
        where high-level keys like 'assetAdministrationShells', 'submodels' or 'conceptDescriptions' should be skipped.
        """
        return self._process_dict(json_data)

    def _process_json_file(self, file_path: str) -> Tuple[List[Dict], Dict[str, List]]:
        """Process a single JSON file and return nodes and relationships."""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            nodes, relationships = self._process_json_data(data)
        return nodes, relationships

    def _process_json_files_batch(self, directory: str, files_batch: List[str]) -> Tuple[List[Dict], Dict[str, List]]:
        """Process a batch of JSON files and return nodes and relationships."""
        batch_nodes = []
        batch_relationships = {}
        for filename in files_batch:
            nodes, relationships = self._process_json_file(join(directory, filename))
            batch_nodes.extend(nodes)
            self._merge_relationships(batch_relationships, relationships)
        return batch_nodes, batch_relationships

    def upload_all_json_from_dir(self, directory: str, file_batch_size: int = 50,
                                 db_batch_size: int = 10000, max_num_of_batches=10000) -> UploadStats:
        """Upload JSON files from directory into Neo4j using batch processing."""
        stats = UploadStats()
        json_files = [f for f in os.listdir(directory) if isfile(join(directory, f)) and f.endswith('.json')]
        stats.total_files = len(json_files)
        stats.total_batches = (stats.total_files + file_batch_size - 1) // file_batch_size

        logger.info(f"Found {stats.total_files} JSON files in directory '{directory}'")
        logger.info(f"File batch size: {stats.total_batches} batches of {file_batch_size} files each")
        logger.info(f"Database transaction batch size: {db_batch_size}")

        # Process files in batches
        for batch_num in range(stats.total_batches):
            start_idx = batch_num * file_batch_size
            end_idx = min(start_idx + file_batch_size, stats.total_files)
            current_file_batch = json_files[start_idx:end_idx]

            logger.info(f"\n--- Processing Batch {batch_num + 1}/{stats.total_batches} ---")
            logger.info(f"Files {start_idx + 1}-{end_idx} of {stats.total_files}")

            # Process current batch
            batch_start_time = time.time()
            batch_nodes, batch_relationships = self._process_json_files_batch(directory, current_file_batch)

            processing_time = time.time() - batch_start_time
            stats.total_processing_time += processing_time
            logger.info(f"Processed {len(current_file_batch)} files in {processing_time:.2f} seconds")

            if not batch_nodes:
                logger.info("No nodes to create in this batch, skipping...")
                continue

            self._upload_nodes_and_relationships(batch_nodes, batch_relationships, stats, db_batch_size=db_batch_size)

            batch_total_time = time.time() - batch_start_time
            logger.info(f"Batch {batch_num + 1} completed in {batch_total_time:.2f} seconds")

            if batch_num == max_num_of_batches:
                logger.warning("Max number of batches reached")
                break

        stats.finish()
        return stats

    def upload_json_file(self, file_path: str, db_batch_size: int = 1000):
        nodes, relationships = self._process_json_file(file_path)
        stats = self._upload_nodes_and_relationships(nodes, relationships, db_batch_size=db_batch_size)
        stats.finish()

    def upload_json(self, json_data: Dict[str, Any], db_batch_size: int = 1000):
        nodes, relationships = self._process_json_data(json_data)
        stats = self._upload_nodes_and_relationships(nodes, relationships, db_batch_size=db_batch_size)
        stats.finish()
